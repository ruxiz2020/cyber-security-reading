{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://towardsdatascience.com/is-bigger-also-smarter-open-ai-releases-gpt-3-language-model-adbb8b3b8126\n"
     ]
    }
   ],
   "source": [
    "# data collection\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class Url2Text:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Extract text data from url\n",
    "        \"\"\"\n",
    "        self._url = url\n",
    "\n",
    "        response = requests.get(self._url)\n",
    "        data = response.content.decode('utf-8', errors=\"replace\")\n",
    "\n",
    "        self._soup = BeautifulSoup(data, \"lxml\")\n",
    "\n",
    "    def _get_title(self):\n",
    "\n",
    "        try:\n",
    "            title_ = self._soup.find('title')\n",
    "            title = title_.getText()\n",
    "        except:\n",
    "            try:\n",
    "                title_ = self._soup.find('h1')\n",
    "                title = title_.getText()\n",
    "            except:\n",
    "                title = \"NA\"\n",
    "        return title\n",
    "\n",
    "    def extract_text_from_html(self):\n",
    "\n",
    "        title = self._get_title()\n",
    "        #try:\n",
    "        #    title = title_.getText()\n",
    "        #except:\n",
    "        #    title = \"title\"\n",
    "\n",
    "        page = self._soup.findAll('p') # works for wikipedia\n",
    "        #page = self._soup.findAll([\"div\", {\"class\" : re.compile('*paragraph*')},]) # works for cnn news\n",
    "\n",
    "        sentences = []\n",
    "        for pp in page: # loop over all <p> sentence</p>\n",
    "\n",
    "            text = pp.getText().strip()\n",
    "\n",
    "            sentences.append(text)\n",
    "            # print(text)\n",
    "\n",
    "        #paragraph = ' '.join(sentences)\n",
    "        return title, sentences\n",
    "\n",
    "    \n",
    "def write_to_file(lines, filename):\n",
    "    \n",
    "    with open(filename, \"w\") as fhandle:\n",
    "        for line in lines:\n",
    "            fhandle.write(f'{line}\\n')\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    url = \"https://towardsdatascience.com/is-bigger-also-smarter-open-ai-releases-gpt-3-language-model-adbb8b3b8126\"\n",
    "    print(url)\n",
    "\n",
    "    extractText = Url2Text(url)\n",
    "    title, sentences = extractText.extract_text_from_html()\n",
    "    \n",
    "    #dict_text = {}\n",
    "    #dict_text[\"url\"] = url\n",
    "    #dict_text[\"title\"] = title\n",
    "    #dict_text[\"text\"] = sentences\n",
    "    \n",
    "    sentences = [title] + [url] + sentences \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is bigger also smarter? — Open AI releases GPT-3 language model | by Andreas Stöckl | Towards Data Science',\n",
       " 'https://towardsdatascience.com/is-bigger-also-smarter-open-ai-releases-gpt-3-language-model-adbb8b3b8126',\n",
       " 'Progress in NLP applications is driven by larger language models consisting of neural networks using the Transformer Architecture. On the occasion of the recently published results of the currently largest model — GPT-3 of Open AI, I would like to take a closer look at these advances.',\n",
       " 'On May 28, 2020, a paper (https://arxiv.org/abs/2005.14165) by OpenAI researchers was published on ArXiv about GPT-3, a language model that is capable of achieving good results in a number of benchmark language processing tasks ranging from language translation and news article writing to question answering. The special thing about it is that these results are achieved without fine-tuning for the benchmark under consideration, but are achieved by the language model without any further information (“zero-shot”) or with little additional information (“one-shot” or “few-shot”).',\n",
       " 'GPT-3 has no less than 175 billion parameters. For comparison, the largest version of GPT-2 had 1.5 billion parameters, and the world’s largest transformer-based language model — introduced by Microsoft earlier this month — has 17 billion parameters.',\n",
       " 'When Open AI released the previous generation GPT-2 a year ago with great media echo, this was also due to the announcement not to release the largest model, because it could generate harmful things like fake news. Meanwhile, the generation of news from a given context with such models is nothing special anymore. However, the generated articles are not just pieced together from parts of articles from the training data, but are really new, as experiments have shown.',\n",
       " 'At first glance, the contributions are often indistinguishable from real news. In the current GPT-3 paper, one result is that generated news articles can hardly be distinguished from real news by people. In one experiment only 52% were correctly recognized, which is close to 50% that would be achieved by dicing. This could also be due to the fact that some real news items are so absurd that the dividing line can no longer be drawn sharply.',\n",
       " 'As my own experiments with stories about Austria have shown,',\n",
       " 'the articles are largely syntactically correct and read pleasingly, and the facts sound quite logical, but are often recognizable as incorrect when one is familiar with the facts.',\n",
       " 'It seems as if the model filters matching articles from a kind of “memory” through the given context, and then “interpolates” them into a new story. The memory is formed by the high number of parameters that are optimized when training with text data. GPT-3 was trained on the CommonCrawl dataset of almost one trillion words collected between 2016 and 2019, as well as on datasets related to web texts, books, and Wikipedia.',\n",
       " 'This reminds me of the saying that DeepLearning is just glorified “curve fitting”, which I discussed in an earlier article:',\n",
       " 'But it is not really “understanding” texts or even drawing conclusions from texts. As the results in the new paper also show, GPT-3 models have a hard time, especially in tests that require real conclusions.',\n",
       " 'An interesting experiment in the paper is also the attempt to calculate examples given as text, for example:',\n",
       " '“ What is 48 plus 76? ”',\n",
       " 'to be calculated by the model. It is shown here that the largest model (175 billion parameters) allows simple addition and subtraction of two or three numbers, but not yet longer calculations. And that again without special fine-tuning for this task!',\n",
       " 'As if the size of the “lookup table” was large enough for the shorter calculations, but not for the longer ones. Where does the data for these “tables “come from?',\n",
       " 'In the texts from the internet, which were used for the training of GPT-3, there is enough material for this. An Internet search for the calculations provides tables like the following one:',\n",
       " 'The question arises for me if further scaling up of models and data will lead to further substantial improvements for NLP applications, or if it is time for new ideas and approaches?',\n",
       " 'Hands-on real-world examples, research,  tutorials, and cutting-edge techniques delivered Monday to Thursday. Make learning your daily ritual.\\xa0Take a look',\n",
       " 'Written by',\n",
       " 'Written by']"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(sentences, filename=url.split(\"/\")[-1]+\".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from cStringIO import StringIO\n",
    "\n",
    "def pdfparser(data):\n",
    "\n",
    "    fp = file(data, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    # Process each page contained in the document.\n",
    "\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "\n",
    "    print data\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    pdfparser(sys.argv[1])  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
